{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker exec -it namenode bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09656a51",
   "metadata": {},
   "source": [
    "Raw Layer\n",
    "\n",
    "Purpose: Store the original data exactly as scraped from sources.\n",
    "If something goes wrong downstream (bad transformations, column mismatch), you can reprocess from raw without re-scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hdfs dfs -mkdir -p /datalake/bronze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6382d",
   "metadata": {},
   "source": [
    "Bronze Layer (RAW DATA) after saving raw data we will:\n",
    "\n",
    ": Standardize and clean data.\n",
    "\n",
    "Fix inconsistent column names.\n",
    "\n",
    "Convert types (string → integer/float/date).\n",
    "\n",
    "Remove obvious corrupt rows.\n",
    "\n",
    "Separates cleaning concerns from business transformations.\n",
    "\n",
    "Downstream layers can trust the bronze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440deb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdfs dfs -mkdir -p /datalake/silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766baf02",
   "metadata": {},
   "source": [
    "Silver Layer (Transforming)\n",
    "\n",
    "Purpose: Combine multiple sources and apply business logic.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Merge Dubizzle, Bayut, FazWaz data into a unified schema.\n",
    "\n",
    "Calculate derived columns (price per m², property age, etc.).\n",
    "\n",
    "Filter for relevant data.\n",
    "\n",
    "Why:\n",
    "\n",
    "Silver data is ready for analytics or machine learning.\n",
    "\n",
    "Avoids mixing cleaning and business transformations in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir -p /datalake/gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b940558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -chmod 777 /datalake/bronze\n",
    "# hdfs dfs -chmod 777 /datalake/silver\n",
    "# hdfs dfs -chmod 777 /datalake/gold  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5ca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data/data_csv_files/bayut/bayutData.csv...\n",
      "Saved hdfs://namenode:9000/datalake/bronze/bayutData successfully.\n",
      "Processing /data/data_csv_files/dubbizle/dubbizle_alexandria.csv...\n",
      "Saved hdfs://namenode:9000/datalake/bronze/dubbizle_alexandria successfully.\n",
      "Processing /data/data_csv_files/dubbizle/dubizzle_all_listings_cairo.csv...\n",
      "Saved hdfs://namenode:9000/datalake/bronze/dubizzle_all_listings_cairo successfully.\n",
      "Processing /data/data_csv_files/fazwaz.com/fazwaz_apartments_allcombined.csv...\n",
      "Saved hdfs://namenode:9000/datalake/bronze/fazwaz_apartments_allcombined successfully.\n",
      "Processing /data/data_csv_files/propertyfinder/propertyfinder.csv...\n",
      "Saved hdfs://namenode:9000/datalake/bronze/propertyfinder successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVtoParquetCustomNames\").getOrCreate()\n",
    "\n",
    "# Mapping of local CSV files to HDFS parquet filenames\n",
    "files_mapping = {\n",
    "    \"/data/data_csv_files/bayut/bayutData.csv\": \"bayutData\",\n",
    "    \"/data/data_csv_files/dubbizle/dubbizle_alexandria.csv\": \"dubbizle_alexandria\",\n",
    "    \"/data/data_csv_files/dubbizle/dubizzle_all_listings_cairo.csv\": \"dubizzle_all_listings_cairo\",\n",
    "    \"/data/data_csv_files/fazwaz/fazwaz_apartments_allcombined.csv\": \"fazwaz_apartments_allcombined\",\n",
    "    \"/data/data_csv_files/propertyfinder/propertyfinder.csv\": \"propertyfinder\"\n",
    "}\n",
    "\n",
    "hdfs_bronze_path = \"hdfs://namenode:9000/datalake/bronze/\"\n",
    "\n",
    "for local_csv, parquet_name in files_mapping.items():\n",
    "    print(f\"Processing {local_csv}...\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = spark.read.option(\"header\", True).csv(local_csv)\n",
    "    \n",
    "    # Write to HDFS as parquet with the mapped name\n",
    "    parquet_path = f\"{hdfs_bronze_path}{parquet_name}\"\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    \n",
    "    print(f\"Saved {parquet_path} successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a100504",
   "metadata": {},
   "source": [
    "change permission and give access to all user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a6002",
   "metadata": {},
   "source": [
    "<!-- hdfs dfs -chmod 777 /datalake/bronze\n",
    "hdfs dfs -chmod 777 /datalake/gold\n",
    "hdfs dfs -chmod 777 /datalake/raw\n",
    "hdfs dfs -chmod 777 /datalake/silver -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
