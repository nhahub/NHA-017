[2025-11-30T19:15:18.155+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-11-30T19:15:18.172+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: real_estate_etl_pipeline.ingest_to_bronze manual__2025-11-30T18:25:30.102845+00:00 [queued]>
[2025-11-30T19:15:18.177+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: real_estate_etl_pipeline.ingest_to_bronze manual__2025-11-30T18:25:30.102845+00:00 [queued]>
[2025-11-30T19:15:18.178+0000] {taskinstance.py:2306} INFO - Starting attempt 3 of 4
[2025-11-30T19:15:18.188+0000] {taskinstance.py:2330} INFO - Executing <Task(BashOperator): ingest_to_bronze> on 2025-11-30 18:25:30.102845+00:00
[2025-11-30T19:15:18.195+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=667) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-11-30T19:15:18.197+0000] {standard_task_runner.py:63} INFO - Started process 669 to run task
[2025-11-30T19:15:18.197+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'real_estate_etl_pipeline', 'ingest_to_bronze', 'manual__2025-11-30T18:25:30.102845+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/scraping_dag.py', '--cfg-path', '/tmp/tmpsq8qdj1o']
[2025-11-30T19:15:18.199+0000] {standard_task_runner.py:91} INFO - Job 42: Subtask ingest_to_bronze
[2025-11-30T19:15:18.237+0000] {task_command.py:426} INFO - Running <TaskInstance: real_estate_etl_pipeline.ingest_to_bronze manual__2025-11-30T18:25:30.102845+00:00 [running]> on host 9a9a5f972fcd
[2025-11-30T19:15:18.306+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='real_estate_etl_pipeline' AIRFLOW_CTX_TASK_ID='ingest_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-11-30T18:25:30.102845+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-30T18:25:30.102845+00:00'
[2025-11-30T19:15:18.308+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-11-30T19:15:18.309+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-11-30T19:15:18.311+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'docker exec jupyter spark-submit --master local[*] /data/ingest_to_bronze.py']
[2025-11-30T19:15:18.322+0000] {subprocess.py:86} INFO - Output:
[2025-11-30T19:15:20.207+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkContext: Running Spark version 3.5.0
[2025-11-30T19:15:20.209+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-11-30T19:15:20.209+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkContext: Java version 17.0.8.1
[2025-11-30T19:15:20.240+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-11-30T19:15:20.299+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceUtils: ==============================================================
[2025-11-30T19:15:20.300+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-11-30T19:15:20.301+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceUtils: ==============================================================
[2025-11-30T19:15:20.301+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkContext: Submitted application: IngestToBronze
[2025-11-30T19:15:20.313+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-11-30T19:15:20.319+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceProfile: Limiting resource is cpu
[2025-11-30T19:15:20.319+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-11-30T19:15:20.348+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SecurityManager: Changing view acls to: jovyan
[2025-11-30T19:15:20.349+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SecurityManager: Changing modify acls to: jovyan
[2025-11-30T19:15:20.349+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SecurityManager: Changing view acls groups to:
[2025-11-30T19:15:20.350+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SecurityManager: Changing modify acls groups to:
[2025-11-30T19:15:20.350+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jovyan; groups with view permissions: EMPTY; users with modify permissions: jovyan; groups with modify permissions: EMPTY
[2025-11-30T19:15:20.497+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Utils: Successfully started service 'sparkDriver' on port 36743.
[2025-11-30T19:15:20.514+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkEnv: Registering MapOutputTracker
[2025-11-30T19:15:20.540+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkEnv: Registering BlockManagerMaster
[2025-11-30T19:15:20.550+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-11-30T19:15:20.551+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-11-30T19:15:20.553+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-11-30T19:15:20.567+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-197205b4-0b67-454b-a75d-d2e3cd903454
[2025-11-30T19:15:20.576+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-11-30T19:15:20.588+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-11-30T19:15:20.670+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-11-30T19:15:20.708+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-11-30T19:15:20.777+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Executor: Starting executor ID driver on host 76c6f5c40d8c
[2025-11-30T19:15:20.777+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-11-30T19:15:20.778+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Executor: Java version 17.0.8.1
[2025-11-30T19:15:20.781+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-11-30T19:15:20.782+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@57061531 for default.
[2025-11-30T19:15:20.794+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46843.
[2025-11-30T19:15:20.795+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO NettyBlockTransferService: Server created on 76c6f5c40d8c:46843
[2025-11-30T19:15:20.795+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-11-30T19:15:20.799+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 76c6f5c40d8c, 46843, None)
[2025-11-30T19:15:20.802+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManagerMasterEndpoint: Registering block manager 76c6f5c40d8c:46843 with 434.4 MiB RAM, BlockManagerId(driver, 76c6f5c40d8c, 46843, None)
[2025-11-30T19:15:20.804+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 76c6f5c40d8c, 46843, None)
[2025-11-30T19:15:20.804+0000] {subprocess.py:93} INFO - 25/11/30 19:15:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 76c6f5c40d8c, 46843, None)
[2025-11-30T19:15:21.027+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:21.027+0000] {subprocess.py:93} INFO - Processing /data/data_csv_files/bayut/bayut_final.csv...
[2025-11-30T19:15:21.072+0000] {subprocess.py:93} INFO - 25/11/30 19:15:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-11-30T19:15:21.077+0000] {subprocess.py:93} INFO - 25/11/30 19:15:21 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.
[2025-11-30T19:15:21.687+0000] {subprocess.py:93} INFO - 25/11/30 19:15:21 INFO InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.
[2025-11-30T19:15:21.766+0000] {subprocess.py:93} INFO - 25/11/30 19:15:21 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-11-30T19:15:23.344+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:23.345+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2025-11-30T19:15:23.727+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO CodeGenerator: Code generated in 140.028602 ms
[2025-11-30T19:15:23.757+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.8 KiB, free 434.2 MiB)
[2025-11-30T19:15:23.800+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2025-11-30T19:15:23.803+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.4 MiB)
[2025-11-30T19:15:23.806+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:23.813+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:23.891+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:23.902+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:23.902+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:23.903+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:23.903+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:23.904+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:23.948+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
[2025-11-30T19:15:23.954+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
[2025-11-30T19:15:23.955+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 76c6f5c40d8c:46843 (size: 6.4 KiB, free: 434.4 MiB)
[2025-11-30T19:15:23.955+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:23.965+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:23.965+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-11-30T19:15:23.989+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8227 bytes)
[2025-11-30T19:15:23.998+0000] {subprocess.py:93} INFO - 25/11/30 19:15:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-11-30T19:15:24.072+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodeGenerator: Code generated in 10.97957 ms
[2025-11-30T19:15:24.074+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/bayut/bayut_final.csv, range: 0-508424, partition values: [empty row]
[2025-11-30T19:15:24.089+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodeGenerator: Code generated in 9.998743 ms
[2025-11-30T19:15:24.160+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1716 bytes result sent to driver
[2025-11-30T19:15:24.167+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 184 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:24.168+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-11-30T19:15:24.171+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.261 s
[2025-11-30T19:15:24.173+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:24.174+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-11-30T19:15:24.175+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.284102 s
[2025-11-30T19:15:24.190+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodeGenerator: Code generated in 7.881895 ms
[2025-11-30T19:15:24.226+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:24.227+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:24.231+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)
[2025-11-30T19:15:24.242+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)
[2025-11-30T19:15:24.245+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:24.246+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:24.247+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:24.252+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 76c6f5c40d8c:46843 in memory (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:24.577+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:24.578+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:24.664+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:24.685+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:24.685+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:24.686+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:24.687+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:24.687+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:24.687+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:24.698+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.5 KiB, free 433.7 MiB)
[2025-11-30T19:15:24.710+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)
[2025-11-30T19:15:24.711+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:24.712+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SparkContext: Created broadcast 3 from parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:24.716+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:24.725+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:24.727+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:24.727+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:24.728+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:24.728+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:24.728+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:24.750+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.7 KiB, free 433.5 MiB)
[2025-11-30T19:15:24.756+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 433.4 MiB)
[2025-11-30T19:15:24.758+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 76c6f5c40d8c:46843 (size: 77.4 KiB, free: 434.2 MiB)
[2025-11-30T19:15:24.759+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:24.760+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:24.761+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-11-30T19:15:24.762+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8227 bytes)
[2025-11-30T19:15:24.764+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-11-30T19:15:24.796+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:24.797+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:24.798+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:24.798+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:24.799+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:24.800+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:24.806+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:24.807+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:24.820+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-11-30T19:15:24.840+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-11-30T19:15:24.841+0000] {subprocess.py:93} INFO - {
[2025-11-30T19:15:24.841+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-11-30T19:15:24.842+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-11-30T19:15:24.842+0000] {subprocess.py:93} INFO -     "name" : "title",
[2025-11-30T19:15:24.843+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.843+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.844+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.844+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.844+0000] {subprocess.py:93} INFO -     "name" : "price",
[2025-11-30T19:15:24.845+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.845+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.846+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.846+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.847+0000] {subprocess.py:93} INFO -     "name" : "currency",
[2025-11-30T19:15:24.847+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.847+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.848+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.848+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.848+0000] {subprocess.py:93} INFO -     "name" : "area_sqm",
[2025-11-30T19:15:24.849+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.849+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.850+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.850+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.850+0000] {subprocess.py:93} INFO -     "name" : "location",
[2025-11-30T19:15:24.850+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.851+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.851+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.852+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.852+0000] {subprocess.py:93} INFO -     "name" : "bedrooms",
[2025-11-30T19:15:24.852+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.853+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.853+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.854+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.854+0000] {subprocess.py:93} INFO -     "name" : "bathrooms",
[2025-11-30T19:15:24.854+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.854+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.855+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.855+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:24.855+0000] {subprocess.py:93} INFO -     "name" : "url",
[2025-11-30T19:15:24.856+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:24.856+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:24.856+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:24.857+0000] {subprocess.py:93} INFO -   } ]
[2025-11-30T19:15:24.857+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:24.857+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-11-30T19:15:24.858+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-11-30T19:15:24.858+0000] {subprocess.py:93} INFO -   optional binary title (STRING);
[2025-11-30T19:15:24.858+0000] {subprocess.py:93} INFO -   optional binary price (STRING);
[2025-11-30T19:15:24.859+0000] {subprocess.py:93} INFO -   optional binary currency (STRING);
[2025-11-30T19:15:24.859+0000] {subprocess.py:93} INFO -   optional binary area_sqm (STRING);
[2025-11-30T19:15:24.860+0000] {subprocess.py:93} INFO -   optional binary location (STRING);
[2025-11-30T19:15:24.860+0000] {subprocess.py:93} INFO -   optional binary bedrooms (STRING);
[2025-11-30T19:15:24.860+0000] {subprocess.py:93} INFO -   optional binary bathrooms (STRING);
[2025-11-30T19:15:24.861+0000] {subprocess.py:93} INFO -   optional binary url (STRING);
[2025-11-30T19:15:24.861+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:24.862+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:24.862+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:24.886+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-11-30T19:15:25.000+0000] {subprocess.py:93} INFO - 25/11/30 19:15:24 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/bayut/bayut_final.csv, range: 0-508424, partition values: [empty row]
[2025-11-30T19:15:25.025+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO CodeGenerator: Code generated in 17.387495 ms
[2025-11-30T19:15:25.559+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileOutputCommitter: Saved output of task 'attempt_202511301915243774785078207402469_0001_m_000000_1' to hdfs://namenode:9000/datalake/bronze/bayutData/_temporary/0/task_202511301915243774785078207402469_0001_m_000000
[2025-11-30T19:15:25.560+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkHadoopMapRedUtil: attempt_202511301915243774785078207402469_0001_m_000000_1: Committed. Elapsed time: 11 ms.
[2025-11-30T19:15:25.572+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2545 bytes result sent to driver
[2025-11-30T19:15:25.574+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 812 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:25.574+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-11-30T19:15:25.575+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.847 s
[2025-11-30T19:15:25.576+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:25.577+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-11-30T19:15:25.577+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.851601 s
[2025-11-30T19:15:25.579+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileFormatWriter: Start to commit write Job cd446d94-23af-44ce-ae31-729d2ce74523.
[2025-11-30T19:15:25.603+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileFormatWriter: Write Job cd446d94-23af-44ce-ae31-729d2ce74523 committed. Elapsed time: 23 ms.
[2025-11-30T19:15:25.606+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileFormatWriter: Finished processing stats for write job cd446d94-23af-44ce-ae31-729d2ce74523.
[2025-11-30T19:15:25.608+0000] {subprocess.py:93} INFO - âœ… Saved to hdfs://namenode:9000/datalake/bronze/bayutData
[2025-11-30T19:15:25.608+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:25.608+0000] {subprocess.py:93} INFO - Processing /data/data_csv_files/dubbizle/dubizzle_all_listings_unlimited_pages.csv...
[2025-11-30T19:15:25.652+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.
[2025-11-30T19:15:25.687+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-11-30T19:15:25.753+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:25.753+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#49, None)) > 0)
[2025-11-30T19:15:25.765+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.8 KiB, free 433.2 MiB)
[2025-11-30T19:15:25.773+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.2 MiB)
[2025-11-30T19:15:25.775+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:25.776+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Created broadcast 5 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:25.777+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:25.785+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:25.786+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:25.787+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:25.787+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:25.788+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:25.788+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:25.791+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.5 KiB, free 433.2 MiB)
[2025-11-30T19:15:25.799+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.2 MiB)
[2025-11-30T19:15:25.801+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 76c6f5c40d8c:46843 in memory (size: 77.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:25.802+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 76c6f5c40d8c:46843 (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:25.802+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:25.803+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:25.803+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-11-30T19:15:25.806+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8256 bytes)
[2025-11-30T19:15:25.807+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-11-30T19:15:25.813+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/dubbizle/dubizzle_all_listings_unlimited_pages.csv, range: 0-1517543, partition values: [empty row]
[2025-11-30T19:15:25.844+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:25.848+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1620 bytes result sent to driver
[2025-11-30T19:15:25.850+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 45 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:25.851+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-11-30T19:15:25.852+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.063 s
[2025-11-30T19:15:25.852+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:25.852+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-11-30T19:15:25.853+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.067055 s
[2025-11-30T19:15:25.870+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:25.871+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:25.876+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.8 KiB, free 433.3 MiB)
[2025-11-30T19:15:25.886+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 76c6f5c40d8c:46843 in memory (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:25.889+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.3 MiB)
[2025-11-30T19:15:25.889+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:25.890+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Created broadcast 7 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:25.892+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:25.931+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:25.932+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:25.944+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:25.946+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:25.947+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:25.947+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:25.948+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:25.948+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:25.949+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:25.960+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 199.5 KiB, free 433.1 MiB)
[2025-11-30T19:15:25.968+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.0 MiB)
[2025-11-30T19:15:25.971+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:25.972+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Created broadcast 8 from parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:25.974+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:25.981+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:25.982+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:25.983+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:25.983+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:25.984+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:25.984+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:25.994+0000] {subprocess.py:93} INFO - 25/11/30 19:15:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 213.5 KiB, free 432.8 MiB)
[2025-11-30T19:15:26.004+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 77.3 KiB, free 432.7 MiB)
[2025-11-30T19:15:26.006+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.008+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 76c6f5c40d8c:46843 (size: 77.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.009+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:26.011+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:26.011+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-11-30T19:15:26.012+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.013+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8256 bytes)
[2025-11-30T19:15:26.015+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-11-30T19:15:26.032+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.033+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.034+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.035+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.035+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.036+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.036+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:26.037+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:26.039+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-11-30T19:15:26.042+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-11-30T19:15:26.043+0000] {subprocess.py:93} INFO - {
[2025-11-30T19:15:26.044+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-11-30T19:15:26.044+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-11-30T19:15:26.045+0000] {subprocess.py:93} INFO -     "name" : "description",
[2025-11-30T19:15:26.045+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.046+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.047+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.047+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.048+0000] {subprocess.py:93} INFO -     "name" : "link",
[2025-11-30T19:15:26.048+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.049+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.050+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.050+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.051+0000] {subprocess.py:93} INFO -     "name" : "price",
[2025-11-30T19:15:26.053+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.054+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.055+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.056+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.056+0000] {subprocess.py:93} INFO -     "name" : "location",
[2025-11-30T19:15:26.057+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.057+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.058+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.059+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.059+0000] {subprocess.py:93} INFO -     "name" : "bedrooms",
[2025-11-30T19:15:26.060+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.060+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.061+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.061+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.062+0000] {subprocess.py:93} INFO -     "name" : "bathrooms",
[2025-11-30T19:15:26.063+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.063+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.064+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.064+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.065+0000] {subprocess.py:93} INFO -     "name" : "area",
[2025-11-30T19:15:26.065+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.066+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.066+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.067+0000] {subprocess.py:93} INFO -   } ]
[2025-11-30T19:15:26.067+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:26.068+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-11-30T19:15:26.068+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-11-30T19:15:26.069+0000] {subprocess.py:93} INFO -   optional binary description (STRING);
[2025-11-30T19:15:26.069+0000] {subprocess.py:93} INFO -   optional binary link (STRING);
[2025-11-30T19:15:26.070+0000] {subprocess.py:93} INFO -   optional binary price (STRING);
[2025-11-30T19:15:26.070+0000] {subprocess.py:93} INFO -   optional binary location (STRING);
[2025-11-30T19:15:26.070+0000] {subprocess.py:93} INFO -   optional binary bedrooms (STRING);
[2025-11-30T19:15:26.071+0000] {subprocess.py:93} INFO -   optional binary bathrooms (STRING);
[2025-11-30T19:15:26.071+0000] {subprocess.py:93} INFO -   optional binary area (STRING);
[2025-11-30T19:15:26.072+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:26.072+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.073+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.074+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/dubbizle/dubizzle_all_listings_unlimited_pages.csv, range: 0-1517543, partition values: [empty row]
[2025-11-30T19:15:26.084+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO CodeGenerator: Code generated in 23.798015 ms
[2025-11-30T19:15:26.117+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:26.311+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: Saved output of task 'attempt_20251130191525991477944064152754_0003_m_000000_3' to hdfs://namenode:9000/datalake/bronze/dubbizle_alexandria/_temporary/0/task_20251130191525991477944064152754_0003_m_000000
[2025-11-30T19:15:26.312+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkHadoopMapRedUtil: attempt_20251130191525991477944064152754_0003_m_000000_3: Committed. Elapsed time: 7 ms.
[2025-11-30T19:15:26.314+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2459 bytes result sent to driver
[2025-11-30T19:15:26.316+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 304 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:26.317+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-11-30T19:15:26.318+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.333 s
[2025-11-30T19:15:26.318+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:26.319+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-11-30T19:15:26.319+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.337597 s
[2025-11-30T19:15:26.320+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Start to commit write Job 8b6ff5c6-8e92-4ce0-a59e-943341e778b7.
[2025-11-30T19:15:26.345+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Write Job 8b6ff5c6-8e92-4ce0-a59e-943341e778b7 committed. Elapsed time: 24 ms.
[2025-11-30T19:15:26.346+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Finished processing stats for write job 8b6ff5c6-8e92-4ce0-a59e-943341e778b7.
[2025-11-30T19:15:26.346+0000] {subprocess.py:93} INFO - âœ… Saved to hdfs://namenode:9000/datalake/bronze/dubbizle_alexandria
[2025-11-30T19:15:26.347+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.347+0000] {subprocess.py:93} INFO - Processing /data/data_csv_files/dubbizle/dubizzle_all_listings_cairo.csv...
[2025-11-30T19:15:26.395+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
[2025-11-30T19:15:26.427+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-11-30T19:15:26.497+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:26.498+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#94, None)) > 0)
[2025-11-30T19:15:26.512+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.8 KiB, free 433.0 MiB)
[2025-11-30T19:15:26.521+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 76c6f5c40d8c:46843 in memory (size: 77.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:26.525+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.3 MiB)
[2025-11-30T19:15:26.526+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:26.526+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:26.527+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 10 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:26.528+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:26.535+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:26.537+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Got job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:26.537+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:26.538+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:26.538+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:26.539+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:26.544+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.5 KiB, free 433.5 MiB)
[2025-11-30T19:15:26.546+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.5 MiB)
[2025-11-30T19:15:26.548+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 76c6f5c40d8c:46843 (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:26.550+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:26.552+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:26.552+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-11-30T19:15:26.554+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes)
[2025-11-30T19:15:26.555+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-11-30T19:15:26.560+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/dubbizle/dubizzle_all_listings_cairo.csv, range: 0-1529254, partition values: [empty row]
[2025-11-30T19:15:26.592+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:26.594+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1620 bytes result sent to driver
[2025-11-30T19:15:26.596+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 43 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:26.596+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-11-30T19:15:26.597+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.056 s
[2025-11-30T19:15:26.597+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:26.598+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-11-30T19:15:26.598+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 4 finished: csv at NativeMethodAccessorImpl.java:0, took 0.062364 s
[2025-11-30T19:15:26.605+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 76c6f5c40d8c:46843 in memory (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:26.612+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:26.613+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:26.617+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 199.8 KiB, free 433.3 MiB)
[2025-11-30T19:15:26.624+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.3 MiB)
[2025-11-30T19:15:26.625+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.626+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 12 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:26.626+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:26.659+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:26.660+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:26.668+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.669+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.669+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.670+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.670+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.671+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.671+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.678+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 199.5 KiB, free 433.1 MiB)
[2025-11-30T19:15:26.686+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.0 MiB)
[2025-11-30T19:15:26.688+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.689+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 13 from parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:26.691+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:26.698+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:26.700+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:26.701+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:26.701+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:26.702+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:26.702+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[38] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:26.710+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 213.5 KiB, free 432.8 MiB)
[2025-11-30T19:15:26.717+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 432.7 MiB)
[2025-11-30T19:15:26.717+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.718+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 76c6f5c40d8c:46843 (size: 77.4 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.719+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:26.719+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[38] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:26.719+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-11-30T19:15:26.720+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes)
[2025-11-30T19:15:26.722+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:26.723+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-11-30T19:15:26.739+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.740+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.741+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.741+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:26.742+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:26.742+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:26.743+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:26.743+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:26.744+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-11-30T19:15:26.744+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-11-30T19:15:26.745+0000] {subprocess.py:93} INFO - {
[2025-11-30T19:15:26.745+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-11-30T19:15:26.746+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-11-30T19:15:26.746+0000] {subprocess.py:93} INFO -     "name" : "description",
[2025-11-30T19:15:26.746+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.747+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.747+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.748+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.748+0000] {subprocess.py:93} INFO -     "name" : "link",
[2025-11-30T19:15:26.749+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.749+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.750+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.751+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.752+0000] {subprocess.py:93} INFO -     "name" : "price",
[2025-11-30T19:15:26.752+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.753+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.753+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.753+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.754+0000] {subprocess.py:93} INFO -     "name" : "location",
[2025-11-30T19:15:26.754+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.755+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.755+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.755+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.756+0000] {subprocess.py:93} INFO -     "name" : "bedrooms",
[2025-11-30T19:15:26.756+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.757+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.757+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.757+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.758+0000] {subprocess.py:93} INFO -     "name" : "bathrooms",
[2025-11-30T19:15:26.758+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.759+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.759+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.760+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:26.760+0000] {subprocess.py:93} INFO -     "name" : "area",
[2025-11-30T19:15:26.761+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:26.761+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:26.761+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:26.762+0000] {subprocess.py:93} INFO -   } ]
[2025-11-30T19:15:26.762+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:26.763+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-11-30T19:15:26.763+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-11-30T19:15:26.763+0000] {subprocess.py:93} INFO -   optional binary description (STRING);
[2025-11-30T19:15:26.764+0000] {subprocess.py:93} INFO -   optional binary link (STRING);
[2025-11-30T19:15:26.764+0000] {subprocess.py:93} INFO -   optional binary price (STRING);
[2025-11-30T19:15:26.765+0000] {subprocess.py:93} INFO -   optional binary location (STRING);
[2025-11-30T19:15:26.765+0000] {subprocess.py:93} INFO -   optional binary bedrooms (STRING);
[2025-11-30T19:15:26.765+0000] {subprocess.py:93} INFO -   optional binary bathrooms (STRING);
[2025-11-30T19:15:26.766+0000] {subprocess.py:93} INFO -   optional binary area (STRING);
[2025-11-30T19:15:26.766+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:26.767+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.767+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.768+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/dubbizle/dubizzle_all_listings_cairo.csv, range: 0-1529254, partition values: [empty row]
[2025-11-30T19:15:26.786+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:26.913+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileOutputCommitter: Saved output of task 'attempt_202511301915262121198154771154713_0005_m_000000_5' to hdfs://namenode:9000/datalake/bronze/dubizzle_all_listings_cairo/_temporary/0/task_202511301915262121198154771154713_0005_m_000000
[2025-11-30T19:15:26.913+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO SparkHadoopMapRedUtil: attempt_202511301915262121198154771154713_0005_m_000000_5: Committed. Elapsed time: 4 ms.
[2025-11-30T19:15:26.914+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2459 bytes result sent to driver
[2025-11-30T19:15:26.915+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 195 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:26.917+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-11-30T19:15:26.917+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.215 s
[2025-11-30T19:15:26.918+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:26.918+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-11-30T19:15:26.919+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.218693 s
[2025-11-30T19:15:26.919+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Start to commit write Job a63677f2-2611-4276-86e5-7f1f76429f94.
[2025-11-30T19:15:26.938+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Write Job a63677f2-2611-4276-86e5-7f1f76429f94 committed. Elapsed time: 19 ms.
[2025-11-30T19:15:26.939+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO FileFormatWriter: Finished processing stats for write job a63677f2-2611-4276-86e5-7f1f76429f94.
[2025-11-30T19:15:26.939+0000] {subprocess.py:93} INFO - âœ… Saved to hdfs://namenode:9000/datalake/bronze/dubizzle_all_listings_cairo
[2025-11-30T19:15:26.940+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:26.940+0000] {subprocess.py:93} INFO - Processing /data/data_csv_files/fazwaz/fazwaz_apartments_allcombined.csv...
[2025-11-30T19:15:26.985+0000] {subprocess.py:93} INFO - 25/11/30 19:15:26 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-11-30T19:15:27.012+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2025-11-30T19:15:27.065+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.066+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#139, None)) > 0)
[2025-11-30T19:15:27.074+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 199.8 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.083+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 76c6f5c40d8c:46843 in memory (size: 77.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:27.085+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.3 MiB)
[2025-11-30T19:15:27.085+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.086+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 15 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.086+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.087+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2025-11-30T19:15:27.096+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.097+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:27.099+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:27.099+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:27.100+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:27.100+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[42] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:27.103+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 13.5 KiB, free 433.5 MiB)
[2025-11-30T19:15:27.106+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.5 MiB)
[2025-11-30T19:15:27.107+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 76c6f5c40d8c:46843 (size: 6.4 KiB, free: 434.3 MiB)
[2025-11-30T19:15:27.107+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:27.107+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[42] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:27.108+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-11-30T19:15:27.109+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes)
[2025-11-30T19:15:27.109+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-11-30T19:15:27.113+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/fazwaz/fazwaz_apartments_allcombined.csv, range: 0-2767737, partition values: [empty row]
[2025-11-30T19:15:27.143+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:27.146+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1732 bytes result sent to driver
[2025-11-30T19:15:27.146+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 37 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:27.147+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-11-30T19:15:27.148+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.046 s
[2025-11-30T19:15:27.148+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:27.149+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-11-30T19:15:27.149+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 0.052321 s
[2025-11-30T19:15:27.158+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.159+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:27.161+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 199.8 KiB, free 433.3 MiB)
[2025-11-30T19:15:27.168+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.2 MiB)
[2025-11-30T19:15:27.169+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.170+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 17 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.170+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.205+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.206+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:27.218+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.220+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.221+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.221+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.221+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.222+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.222+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.225+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 199.5 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.231+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.231+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.232+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 18 from parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.234+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.239+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.240+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:27.241+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:27.241+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:27.241+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:27.242+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[51] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:27.256+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 216.4 KiB, free 432.8 MiB)
[2025-11-30T19:15:27.266+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.9 KiB, free 432.7 MiB)
[2025-11-30T19:15:27.267+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 76c6f5c40d8c:46843 in memory (size: 6.4 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.267+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 76c6f5c40d8c:46843 (size: 77.9 KiB, free: 434.1 MiB)
[2025-11-30T19:15:27.268+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:27.269+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[51] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:27.269+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-11-30T19:15:27.270+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.270+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes)
[2025-11-30T19:15:27.271+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-11-30T19:15:27.275+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.286+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.287+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.288+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.288+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.288+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.289+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.289+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:27.290+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:27.290+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-11-30T19:15:27.293+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-11-30T19:15:27.294+0000] {subprocess.py:93} INFO - {
[2025-11-30T19:15:27.294+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-11-30T19:15:27.295+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-11-30T19:15:27.295+0000] {subprocess.py:93} INFO -     "name" : "unit_id",
[2025-11-30T19:15:27.296+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.296+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.297+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.297+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.297+0000] {subprocess.py:93} INFO -     "name" : "name",
[2025-11-30T19:15:27.298+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.298+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.299+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.299+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.299+0000] {subprocess.py:93} INFO -     "name" : "price",
[2025-11-30T19:15:27.300+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.300+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.301+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.301+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.302+0000] {subprocess.py:93} INFO -     "name" : "about",
[2025-11-30T19:15:27.302+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.302+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.303+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.303+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.304+0000] {subprocess.py:93} INFO -     "name" : "bedrooms",
[2025-11-30T19:15:27.304+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.305+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.305+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.306+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.306+0000] {subprocess.py:93} INFO -     "name" : "bathrooms",
[2025-11-30T19:15:27.307+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.307+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.307+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.308+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.308+0000] {subprocess.py:93} INFO -     "name" : "size",
[2025-11-30T19:15:27.309+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.309+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.310+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.310+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.311+0000] {subprocess.py:93} INFO -     "name" : "property_type",
[2025-11-30T19:15:27.312+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.312+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.313+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.314+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.314+0000] {subprocess.py:93} INFO -     "name" : "location",
[2025-11-30T19:15:27.314+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.315+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.315+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.316+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.317+0000] {subprocess.py:93} INFO -     "name" : "price_per_sqm",
[2025-11-30T19:15:27.318+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.319+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.319+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.320+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.321+0000] {subprocess.py:93} INFO -     "name" : "link",
[2025-11-30T19:15:27.322+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.322+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.323+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.323+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.324+0000] {subprocess.py:93} INFO -     "name" : "Private Pool",
[2025-11-30T19:15:27.324+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.324+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.325+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.325+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.325+0000] {subprocess.py:93} INFO -     "name" : "Private Gym",
[2025-11-30T19:15:27.326+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.326+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.327+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.327+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.328+0000] {subprocess.py:93} INFO -     "name" : "Private Garden",
[2025-11-30T19:15:27.328+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.329+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.329+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.330+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.330+0000] {subprocess.py:93} INFO -     "name" : "Covered Parking",
[2025-11-30T19:15:27.330+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.331+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.331+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.331+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.332+0000] {subprocess.py:93} INFO -     "name" : "Maids Quarters",
[2025-11-30T19:15:27.332+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.333+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.333+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.334+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.334+0000] {subprocess.py:93} INFO -     "name" : "Jacuzzi",
[2025-11-30T19:15:27.335+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.335+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.336+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.336+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.336+0000] {subprocess.py:93} INFO -     "name" : "Garden",
[2025-11-30T19:15:27.337+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.337+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.337+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.338+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.338+0000] {subprocess.py:93} INFO -     "name" : "Balcony",
[2025-11-30T19:15:27.338+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.339+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.339+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.339+0000] {subprocess.py:93} INFO -   } ]
[2025-11-30T19:15:27.339+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:27.340+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-11-30T19:15:27.340+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-11-30T19:15:27.340+0000] {subprocess.py:93} INFO -   optional binary unit_id (STRING);
[2025-11-30T19:15:27.341+0000] {subprocess.py:93} INFO -   optional binary name (STRING);
[2025-11-30T19:15:27.341+0000] {subprocess.py:93} INFO -   optional binary price (STRING);
[2025-11-30T19:15:27.342+0000] {subprocess.py:93} INFO -   optional binary about (STRING);
[2025-11-30T19:15:27.342+0000] {subprocess.py:93} INFO -   optional binary bedrooms (STRING);
[2025-11-30T19:15:27.343+0000] {subprocess.py:93} INFO -   optional binary bathrooms (STRING);
[2025-11-30T19:15:27.343+0000] {subprocess.py:93} INFO -   optional binary size (STRING);
[2025-11-30T19:15:27.344+0000] {subprocess.py:93} INFO -   optional binary property_type (STRING);
[2025-11-30T19:15:27.344+0000] {subprocess.py:93} INFO -   optional binary location (STRING);
[2025-11-30T19:15:27.344+0000] {subprocess.py:93} INFO -   optional binary price_per_sqm (STRING);
[2025-11-30T19:15:27.345+0000] {subprocess.py:93} INFO -   optional binary link (STRING);
[2025-11-30T19:15:27.345+0000] {subprocess.py:93} INFO -   optional binary Private Pool (STRING);
[2025-11-30T19:15:27.346+0000] {subprocess.py:93} INFO -   optional binary Private Gym (STRING);
[2025-11-30T19:15:27.346+0000] {subprocess.py:93} INFO -   optional binary Private Garden (STRING);
[2025-11-30T19:15:27.346+0000] {subprocess.py:93} INFO -   optional binary Covered Parking (STRING);
[2025-11-30T19:15:27.347+0000] {subprocess.py:93} INFO -   optional binary Maids Quarters (STRING);
[2025-11-30T19:15:27.347+0000] {subprocess.py:93} INFO -   optional binary Jacuzzi (STRING);
[2025-11-30T19:15:27.348+0000] {subprocess.py:93} INFO -   optional binary Garden (STRING);
[2025-11-30T19:15:27.349+0000] {subprocess.py:93} INFO -   optional binary Balcony (STRING);
[2025-11-30T19:15:27.349+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:27.350+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:27.350+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:27.350+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/fazwaz/fazwaz_apartments_allcombined.csv, range: 0-2767737, partition values: [empty row]
[2025-11-30T19:15:27.351+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodeGenerator: Code generated in 24.60566 ms
[2025-11-30T19:15:27.363+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:27.503+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: Saved output of task 'attempt_202511301915274966893178503324285_0007_m_000000_7' to hdfs://namenode:9000/datalake/bronze/fazwaz_apartments_allcombined/_temporary/0/task_202511301915274966893178503324285_0007_m_000000
[2025-11-30T19:15:27.504+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkHadoopMapRedUtil: attempt_202511301915274966893178503324285_0007_m_000000_7: Committed. Elapsed time: 6 ms.
[2025-11-30T19:15:27.504+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2459 bytes result sent to driver
[2025-11-30T19:15:27.506+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 235 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:27.506+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-11-30T19:15:27.506+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.264 s
[2025-11-30T19:15:27.507+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:27.507+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-11-30T19:15:27.508+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.267590 s
[2025-11-30T19:15:27.508+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Start to commit write Job 12086992-2d40-45b8-82a4-9fdf66ba4824.
[2025-11-30T19:15:27.519+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Write Job 12086992-2d40-45b8-82a4-9fdf66ba4824 committed. Elapsed time: 11 ms.
[2025-11-30T19:15:27.519+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Finished processing stats for write job 12086992-2d40-45b8-82a4-9fdf66ba4824.
[2025-11-30T19:15:27.520+0000] {subprocess.py:93} INFO - âœ… Saved to hdfs://namenode:9000/datalake/bronze/fazwaz_apartments_allcombined
[2025-11-30T19:15:27.520+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:27.520+0000] {subprocess.py:93} INFO - Processing /data/data_csv_files/propertyfinder/propertyfinder.csv...
[2025-11-30T19:15:27.559+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
[2025-11-30T19:15:27.586+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-11-30T19:15:27.623+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.623+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#232, None)) > 0)
[2025-11-30T19:15:27.629+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 199.8 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.635+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.636+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.636+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 20 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.637+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.643+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.643+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Got job 8 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:27.644+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:27.644+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:27.644+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:27.645+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[55] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:27.645+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 13.5 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.646+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.647+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 76c6f5c40d8c:46843 (size: 6.4 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.648+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:27.648+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[55] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:27.649+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-11-30T19:15:27.649+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8239 bytes)
[2025-11-30T19:15:27.650+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-11-30T19:15:27.653+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/propertyfinder/propertyfinder.csv, range: 0-1558976, partition values: [empty row]
[2025-11-30T19:15:27.677+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:27.680+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1713 bytes result sent to driver
[2025-11-30T19:15:27.681+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 32 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:27.682+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-11-30T19:15:27.682+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.038 s
[2025-11-30T19:15:27.682+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:27.683+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-11-30T19:15:27.683+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 8 finished: csv at NativeMethodAccessorImpl.java:0, took 0.039416 s
[2025-11-30T19:15:27.690+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.690+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:27.691+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 199.8 KiB, free 432.8 MiB)
[2025-11-30T19:15:27.699+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 76c6f5c40d8c:46843 in memory (size: 6.4 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.701+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.7 MiB)
[2025-11-30T19:15:27.702+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.1 MiB)
[2025-11-30T19:15:27.702+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 76c6f5c40d8c:46843 in memory (size: 77.9 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.703+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 22 from csv at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.703+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.706+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 76c6f5c40d8c:46843 in memory (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.726+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Pushed Filters:
[2025-11-30T19:15:27.727+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceStrategy: Post-Scan Filters:
[2025-11-30T19:15:27.734+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.735+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.736+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.736+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.737+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.737+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.737+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.740+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 199.5 KiB, free 433.1 MiB)
[2025-11-30T19:15:27.745+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.0 MiB)
[2025-11-30T19:15:27.746+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 76c6f5c40d8c:46843 (size: 34.3 KiB, free: 434.2 MiB)
[2025-11-30T19:15:27.746+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 23 from parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.747+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-11-30T19:15:27.749+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-11-30T19:15:27.750+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-30T19:15:27.751+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0)
[2025-11-30T19:15:27.751+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Parents of final stage: List()
[2025-11-30T19:15:27.752+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Missing parents: List()
[2025-11-30T19:15:27.752+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[64] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-30T19:15:27.762+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 215.7 KiB, free 432.8 MiB)
[2025-11-30T19:15:27.764+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 77.7 KiB, free 432.7 MiB)
[2025-11-30T19:15:27.765+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 76c6f5c40d8c:46843 (size: 77.7 KiB, free: 434.1 MiB)
[2025-11-30T19:15:27.765+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1580
[2025-11-30T19:15:27.766+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[64] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-30T19:15:27.766+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-11-30T19:15:27.766+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (76c6f5c40d8c, executor driver, partition 0, PROCESS_LOCAL, 8239 bytes)
[2025-11-30T19:15:27.767+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-11-30T19:15:27.774+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.774+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.775+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.775+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-11-30T19:15:27.775+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-11-30T19:15:27.776+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-11-30T19:15:27.776+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:27.776+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodecConfig: Compression: SNAPPY
[2025-11-30T19:15:27.777+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-11-30T19:15:27.777+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-11-30T19:15:27.778+0000] {subprocess.py:93} INFO - {
[2025-11-30T19:15:27.778+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-11-30T19:15:27.778+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-11-30T19:15:27.779+0000] {subprocess.py:93} INFO -     "name" : "id",
[2025-11-30T19:15:27.779+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.779+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.780+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.780+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.780+0000] {subprocess.py:93} INFO -     "name" : "title",
[2025-11-30T19:15:27.781+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.781+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.782+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.782+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.783+0000] {subprocess.py:93} INFO -     "name" : "price",
[2025-11-30T19:15:27.783+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.783+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.784+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.784+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.784+0000] {subprocess.py:93} INFO -     "name" : "currency",
[2025-11-30T19:15:27.785+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.785+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.785+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.785+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.786+0000] {subprocess.py:93} INFO -     "name" : "location",
[2025-11-30T19:15:27.786+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.786+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.786+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.787+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.787+0000] {subprocess.py:93} INFO -     "name" : "bedrooms",
[2025-11-30T19:15:27.788+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.788+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.788+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.788+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.788+0000] {subprocess.py:93} INFO -     "name" : "bathrooms",
[2025-11-30T19:15:27.789+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.789+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.789+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.790+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.790+0000] {subprocess.py:93} INFO -     "name" : "property_type",
[2025-11-30T19:15:27.790+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.791+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.791+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.791+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.792+0000] {subprocess.py:93} INFO -     "name" : "property_size",
[2025-11-30T19:15:27.792+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.792+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.793+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.793+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.793+0000] {subprocess.py:93} INFO -     "name" : "furnished",
[2025-11-30T19:15:27.794+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.794+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.795+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.795+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.795+0000] {subprocess.py:93} INFO -     "name" : "share_url",
[2025-11-30T19:15:27.796+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.796+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.796+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.797+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.797+0000] {subprocess.py:93} INFO -     "name" : "description",
[2025-11-30T19:15:27.798+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.798+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.798+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.799+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.799+0000] {subprocess.py:93} INFO -     "name" : "amenities",
[2025-11-30T19:15:27.799+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.799+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.800+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.800+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.800+0000] {subprocess.py:93} INFO -     "name" : "listed_date",
[2025-11-30T19:15:27.801+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.801+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.801+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.801+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.802+0000] {subprocess.py:93} INFO -     "name" : "latitude",
[2025-11-30T19:15:27.802+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.802+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.803+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.803+0000] {subprocess.py:93} INFO -   }, {
[2025-11-30T19:15:27.803+0000] {subprocess.py:93} INFO -     "name" : "longitude",
[2025-11-30T19:15:27.803+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-11-30T19:15:27.804+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-11-30T19:15:27.804+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-11-30T19:15:27.804+0000] {subprocess.py:93} INFO -   } ]
[2025-11-30T19:15:27.804+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:27.805+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-11-30T19:15:27.805+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-11-30T19:15:27.805+0000] {subprocess.py:93} INFO -   optional binary id (STRING);
[2025-11-30T19:15:27.805+0000] {subprocess.py:93} INFO -   optional binary title (STRING);
[2025-11-30T19:15:27.805+0000] {subprocess.py:93} INFO -   optional binary price (STRING);
[2025-11-30T19:15:27.806+0000] {subprocess.py:93} INFO -   optional binary currency (STRING);
[2025-11-30T19:15:27.806+0000] {subprocess.py:93} INFO -   optional binary location (STRING);
[2025-11-30T19:15:27.806+0000] {subprocess.py:93} INFO -   optional binary bedrooms (STRING);
[2025-11-30T19:15:27.806+0000] {subprocess.py:93} INFO -   optional binary bathrooms (STRING);
[2025-11-30T19:15:27.806+0000] {subprocess.py:93} INFO -   optional binary property_type (STRING);
[2025-11-30T19:15:27.807+0000] {subprocess.py:93} INFO -   optional binary property_size (STRING);
[2025-11-30T19:15:27.807+0000] {subprocess.py:93} INFO -   optional binary furnished (STRING);
[2025-11-30T19:15:27.807+0000] {subprocess.py:93} INFO -   optional binary share_url (STRING);
[2025-11-30T19:15:27.808+0000] {subprocess.py:93} INFO -   optional binary description (STRING);
[2025-11-30T19:15:27.808+0000] {subprocess.py:93} INFO -   optional binary amenities (STRING);
[2025-11-30T19:15:27.808+0000] {subprocess.py:93} INFO -   optional binary listed_date (STRING);
[2025-11-30T19:15:27.809+0000] {subprocess.py:93} INFO -   optional binary latitude (STRING);
[2025-11-30T19:15:27.809+0000] {subprocess.py:93} INFO -   optional binary longitude (STRING);
[2025-11-30T19:15:27.809+0000] {subprocess.py:93} INFO - }
[2025-11-30T19:15:27.809+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:27.810+0000] {subprocess.py:93} INFO - 
[2025-11-30T19:15:27.810+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileScanRDD: Reading File path: file:///data/data_csv_files/propertyfinder/propertyfinder.csv, range: 0-1558976, partition values: [empty row]
[2025-11-30T19:15:27.810+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO CodeGenerator: Code generated in 14.341642 ms
[2025-11-30T19:15:27.826+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO LineRecordReader: Found UTF-8 BOM and skipped it
[2025-11-30T19:15:27.897+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileOutputCommitter: Saved output of task 'attempt_202511301915272374652845014862951_0009_m_000000_9' to hdfs://namenode:9000/datalake/bronze/propertyfinder/_temporary/0/task_202511301915272374652845014862951_0009_m_000000
[2025-11-30T19:15:27.898+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkHadoopMapRedUtil: attempt_202511301915272374652845014862951_0009_m_000000_9: Committed. Elapsed time: 3 ms.
[2025-11-30T19:15:27.899+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2459 bytes result sent to driver
[2025-11-30T19:15:27.900+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 134 ms on 76c6f5c40d8c (executor driver) (1/1)
[2025-11-30T19:15:27.900+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-11-30T19:15:27.901+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.149 s
[2025-11-30T19:15:27.902+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-30T19:15:27.902+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-11-30T19:15:27.902+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.151459 s
[2025-11-30T19:15:27.903+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Start to commit write Job 70592b65-2faa-4c68-ab83-1066833f6a97.
[2025-11-30T19:15:27.914+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Write Job 70592b65-2faa-4c68-ab83-1066833f6a97 committed. Elapsed time: 12 ms.
[2025-11-30T19:15:27.915+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO FileFormatWriter: Finished processing stats for write job 70592b65-2faa-4c68-ab83-1066833f6a97.
[2025-11-30T19:15:27.916+0000] {subprocess.py:93} INFO - âœ… Saved to hdfs://namenode:9000/datalake/bronze/propertyfinder
[2025-11-30T19:15:27.916+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-11-30T19:15:27.924+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkUI: Stopped Spark web UI at http://76c6f5c40d8c:4040
[2025-11-30T19:15:27.931+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-11-30T19:15:27.940+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO MemoryStore: MemoryStore cleared
[2025-11-30T19:15:27.941+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManager: BlockManager stopped
[2025-11-30T19:15:27.943+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-11-30T19:15:27.947+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-11-30T19:15:27.954+0000] {subprocess.py:93} INFO - 25/11/30 19:15:27 INFO SparkContext: Successfully stopped SparkContext
[2025-11-30T19:15:28.477+0000] {subprocess.py:93} INFO - 25/11/30 19:15:28 INFO ShutdownHookManager: Shutdown hook called
[2025-11-30T19:15:28.477+0000] {subprocess.py:93} INFO - 25/11/30 19:15:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c394e42-72e3-4f35-8cc6-2a934d32b199
[2025-11-30T19:15:28.478+0000] {subprocess.py:93} INFO - 25/11/30 19:15:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-72aca9a6-70ee-411c-b557-39f4a1280ccc
[2025-11-30T19:15:28.480+0000] {subprocess.py:93} INFO - 25/11/30 19:15:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-72aca9a6-70ee-411c-b557-39f4a1280ccc/pyspark-03099896-9e4e-4dc3-ae6f-6f5f8da8e9b7
[2025-11-30T19:15:28.515+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-11-30T19:15:28.515+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-11-30T19:15:28.533+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=real_estate_etl_pipeline, task_id=ingest_to_bronze, run_id=manual__2025-11-30T18:25:30.102845+00:00, execution_date=20251130T182530, start_date=20251130T191518, end_date=20251130T191528
[2025-11-30T19:15:28.568+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-11-30T19:15:28.592+0000] {taskinstance.py:3503} INFO - 4 downstream tasks scheduled from follow-on schedule check
[2025-11-30T19:15:28.595+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
