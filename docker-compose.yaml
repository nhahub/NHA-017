version: "3.8"

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: airflow.Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__WEBSERVER__SECRET_KEY: my_super_secret_123
    AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
    AIRFLOW__SMTP__SMTP_STARTTLS: 'True'
    AIRFLOW__SMTP__SMTP_SSL: 'False'
    AIRFLOW__SMTP__SMTP_PORT: 587
    AIRFLOW__SMTP__SMTP_MAIL_FROM: No mail
    AIRFLOW__SMTP__SMTP_USER: No mail
    AIRFLOW__SMTP__SMTP_PASSWORD: No mail
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock
    - ./shared:/opt/airflow/shared
  user: "root"
  privileged: true
  depends_on:
    redis:
      condition: service_healthy
    postgres-airflow:
      condition: service_healthy

services:

  spark:
    image: apache/spark:3.5.0
    container_name: spark
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./shared:/data

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    depends_on:
      - spark
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark:7077"]
    ports:
      - "8081:8081"
    volumes:
      - ./shared:/data

  jupyter:
    build:
      context: .
      dockerfile: jupyter.Dockerfile
    container_name: jupyter
    depends_on:
      - spark
    dns:
      - 8.8.8.8
      - 8.8.4.4
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_MASTER=spark://spark:7077
    ports:
      - "8888:8888"
    volumes:
      - ./shared:/data
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hdfs-namenode:/hadoop/dfs/name
      - ./shared:/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9864:9864"
    volumes:
      - hdfs-datanode:/hadoop/dfs/data
      - ./shared:/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  postgres_general:
    image: postgres:15
    container_name: postgres_general
    restart: always
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: real_state_dwh
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4:8
    container_name: pgadmin
    restart: always
    depends_on:
      - postgres_general
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8085:80"

  postgres-airflow:
    image: postgres:13
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8080/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    restart: always

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    command: celery worker
    restart: always

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

  flower:
    <<: *airflow-common
    container_name: airflow-flower
    command: celery flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:5555/\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: always

volumes:
  hdfs-namenode:
  hdfs-datanode:
  pgadmin_data:
  postgres_data:
  postgres_airflow_data:
