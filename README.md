<div align="center">

# ğŸ‡ªğŸ‡¬ Egyptian Real Estate Intelligence Market Platform

## Market Analysis â€¢ Recommendation â€¢ AI Advisor

*A unified data platform for transparency & decision-making in Egyptian real estate through big data & AI.*

![Docker](https://img.shields.io/badge/Docker-Containerized-blue?logo=docker)
![Spark](https://img.shields.io/badge/Apache_Spark-Distributed-orange?logo=apachespark)
![Airflow](https://img.shields.io/badge/Airflow-Orchestration-green?logo=apacheairflow)
![Postgres](https://img.shields.io/badge/PostgreSQL-DWH-blue?logo=postgresql)
![AI](https://img.shields.io/badge/AI-RAG_LLM-red)

</div>

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Business Value & Impact](#-business-value--impact)
- [Market Problem & Target Audience](#-market-problem--target-audience)
- [Pipeline Architecture](#-pipeline-architecture)
- [Technical Deep Dive: Medallion Architecture](#-technical-deep-dive-the-medallion-architecture)
- [AI Assistant (RAG) Implementation](#-ai-assistant-rag-implementation)
- [Technology Stack](#-technology-stack--system-components)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Usage Guide](#-usage-guide)
- [Visualizations & Dashboards](#-visualizations--dashboards)

---

## âœ¨ Project Overview

This project delivers a **production-ready, end-to-end data platform** designed to revolutionize the Egyptian real estate market. By implementing a robust data engineering pipeline and advanced machine learning models, we transform raw, fragmented property listings into **actionable market intelligence**, culminating in a personalized property recommendation engine and an intelligent AI assistant.

**Key Features:**

- **Medallion Architecture:** Ensures data quality and reliability across Bronze, Silver, and Gold layers
- **Scalable Processing:** Leverages Apache Spark and Hadoop for distributed data transformation
- **AI-Powered Insights:** Includes a K-Nearest Neighbors (KNN) recommender and an **Ollama RAG Chatbot** for expert advice
- **Full Automation:** Orchestrated by Apache Airflow and containerized with Docker

---

## ğŸ“ˆ Business Value & Impact

This platform addresses key challenges in the Egyptian real estate market by enabling:

- ğŸ“Š Transparent market analysis for buyers and renters
- ğŸ” Accurate region-based price discovery
- ğŸ§­ AI-powered recommendations for property matching
- ğŸ’¼ Investor-ready intelligence reports
- ğŸ— Planning insights for developers and policy makers
- ğŸ¤– Natural language access to data using AI

---

## ğŸ¯ Market Problem & Target Audience

### Problem Statement

The Egyptian real estate sector, despite its rapid growth, suffers from a significant **lack of market transparency**. Key challenges include:

- **Limited Price Visibility:** Difficulty for buyers and investors to determine fair market value due to significant regional price variations and a lack of structured data
- **Absence of Structured Insights:** Stakeholders lack reliable, analytical insights to inform strategic decisions
- **Inefficient Decision-Making:** Buyers struggle to find properties matching their specific needs and budget, while investors lack the market intelligence required for optimal portfolio management

### Target Audience

The insights and tools developed by this project are designed to serve three primary groups:

| Audience | Benefit |
|----------|---------|
| **Home Buyers** | Receive personalized property recommendations and market context to make informed purchasing decisions |
| **Real Estate Investors** | Gain reliable market intelligence, identify "hotspot" and "coldspot" regions, and optimize investment strategies |
| **Policymakers & Developers** | Access structured data and analytical dashboards to inform housing policies and development strategies |

---

## ğŸš€ Pipeline Architecture

The entire system is built around a scalable, automated data pipeline, orchestrated by **Apache Airflow** and containerized with **Docker**. The architecture follows the **Medallion Architecture** (Bronze, Silver, Gold layers) to ensure data quality and reliability at every stage.

<img width="2415" height="1113" alt="Pipeline Diagram" src="https://github.com/user-attachments/assets/0d884238-f718-435b-ac27-508cd318c2dd" />

### Data Orchestration with Airflow

Apache Airflow manages:

- Scraping schedules
- Data processing jobs
- Spark transformations
- Warehouse refresh cycles
- Automated recovery

![Airflow DAG](https://github.com/user-attachments/assets/2f95ae5c-a0e5-4f5b-a4ee-fd0356635060)

---

## âš™ï¸ Technical Deep Dive: The Medallion Architecture

The data flows through three distinct layers, each adding value and refinement.

### 1. ğŸ¥‰ Bronze Layer: Raw Ingestion

| Stage | Tools | Description |
|-------|-------|-------------|
| **Data Sources** | dubizzle, Property Finder, bayut, Fazwaz | Key real estate listing platforms in the Egyptian market |
| **Web Scraper** | BeautifulSoup, Playwright, Python | Automated scripts to navigate and extract property details, handling both static and dynamic content |
| **Raw Ingestion** | **Hadoop Data Lake (Parquet)** | Raw, unprocessed data is ingested directly into the **Bronze Layer**. This layer serves as the immutable historical record, stored in the efficient Parquet format |

### 2. ğŸ¥ˆ Silver Layer: Cleaning & Transformation

| Stage | Tools | Description |
|-------|-------|-------------|
| **Processing Engine** | **Apache Spark, Python** | Used for distributed processing to handle large volumes of data efficiently |
| **Transformation Steps** | Cleaning, Aggregation, Merging | **Cleaning** (null handling, duplicate removal), **Standardization** (unifying Location and deriving region and city to ensure accuracy of results), **Feature Engineering** (generating `price_per_sqm`), and **Merging** data from all sources into a unified schema |
| **Output** | Silver Layer | Contains clean, standardized, and ready-to-be-analyzed datasets |

### 3. ğŸ¥‡ Gold Layer: Data Warehouse & Analytics

| Stage | Tools | Description |
|-------|-------|-------------|
| **Data Warehouse (DWH)** | **PostgreSQL** | A robust, open-source relational database used to store the final, high-value data |
| **Structure** | **One Big Table (OBT)** | All curated listings are loaded into a single, unified table to simplify querying and provide a single source of truth |
| **Data Marts** | Analysis Data Mart, ML Data Mart | Two specialized marts are created from the OBT: one optimized for **Power BI/Superset** reporting and one for the **Machine Learning Model** |
| **ML & BI** | KNN, Flask, Power BI, Superset | The final stage deploys the ML Recommendation Model via Flask and visualizes market trends through BI tools |

### One Big Table DWH:

<img width="2559" height="1396" alt="OBT Schema" src="https://github.com/user-attachments/assets/bb81652b-d4b1-4b7f-963c-218900c2b5fa" />

---

## ğŸ§  AI Assistant (RAG) Implementation

The project features an advanced **AI Assistant Chatbot** built on a **Retrieval-Augmented Generation (RAG)** framework to provide expert, data-driven advice.

### Architecture

- **Framework:** Flask application with a RAG pipeline
- **LLM:** **OLLAMA** (using `llama3`) for intent detection, SQL generation, and final response formatting
- **Retrieval:** **FAISS** index and **Sentence Transformers** (all-MiniLM-L6-v2) are used to retrieve the most relevant property listings based on the user's natural language query
- **Functionality:** The assistant can handle two main intents:
    1. **Recommendation:** Uses RAG to suggest properties based on semantic similarity
    2. **Analytical:** Generates and executes a safe **PostgreSQL SELECT query** against the OBT to answer complex analytical questions (e.g., "What is the average price per square meter in Giza?")

### Logic Flow

<img width="1784" height="911" alt="RAG Logic" src="https://github.com/user-attachments/assets/3271e3cc-3400-4aec-8ace-92009a00eda5" />

### What Makes It Powerful

- Answers with **your private data**
- SQL-safe query generation
- Intent classification
- Hallucination prevention logic
- Recommendation + reporting fusion

Unlike generic chatbots, this AI assistant understands **your warehouse schema**, not just language.

---

## ğŸ› ï¸ Technology Stack & System Components

This project is built on a modern, open-source data stack.

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | **Apache Airflow** | Manages the End-to-End ETL process via a Directed Acyclic Graph (DAG) |
| **Distributed Processing** | **Apache Spark** | High-performance data cleaning and transformation in the Silver Layer |
| **Data Lake** | **Hadoop (HDFS)** | Stores raw (Bronze) and processed (Silver) data in Parquet format |
| **Data Warehouse** | **PostgreSQL** | Stores the final, curated Gold Layer data (OBT and Data Marts) |
| **Web Scraping** | **Python, Playwright, BeautifulSoup** | Automated data extraction from real estate portals |
| **AI/RAG** | **Flask, Ollama, FAISS, Sentence Transformers** | Deploys the AI Assistant and Recommendation Model |
| **BI & Reporting** | **Power BI, Apache Superset** | Interactive dashboards for market trend visualization |
| **Containerization** | **Docker, Docker Compose** | Ensures consistent, reproducible environments for all services |

---

## ğŸ“ Project Structure

```
NHA-017/
â”‚
â”œâ”€â”€ dags/                               # Airflow DAGs for pipeline orchestration
â”‚   â””â”€â”€ main_pipeline_dag.py            # Complete ETL workflow orchestration
â”‚
â”œâ”€â”€ shared/                             # Shared volume - All processing scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ scraping_scripts/               # ğŸ”µ Stage 1: Web Scraping (Bronze Layer)
â”‚   â”‚   â”œâ”€â”€ scrape_propertyfinder.py    # PropertyFinder scraper
â”‚   â”‚   â”œâ”€â”€ scrape_bayut.py             # Bayut scraper
â”‚   â”‚   â”œâ”€â”€ scrape_dubizzle.py          # Dubizzle scraper
â”‚   â”‚   â””â”€â”€ scrape_fazwaz.py            # Fazwaz scraper
â”‚   â”‚
â”‚   â”œâ”€â”€ Cleaning_Layer_Pyspark/         # ğŸŸ¢ Stage 2: Data Cleaning (Silver Layer)
â”‚   â”‚   â”œâ”€â”€ clean_propertyfinder.py     # Clean PropertyFinder data
â”‚   â”‚   â”œâ”€â”€ clean_bayut.py              # Clean Bayut data
â”‚   â”‚   â”œâ”€â”€ clean_dubizzle.py           # Clean Dubizzle data
â”‚   â”‚   â””â”€â”€ clean_fazwaz.py             # Clean Fazwaz data
â”‚   â”‚
â”‚   â”œâ”€â”€ Transformation_Layer_Pyspark/   # ğŸŸ¡ Stage 3: Feature Engineering (Silver Layer)
â”‚   â”‚   â”œâ”€â”€ transform_propertyfinder.py # Transform PropertyFinder data
â”‚   â”‚   â”œâ”€â”€ transform_bayut.py          # Transform Bayut data
â”‚   â”‚   â”œâ”€â”€ transform_dubizzle.py       # Transform Dubizzle data
â”‚   â”‚   â””â”€â”€ transform_fazwaz.py         # Transform Fazwaz data
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest_to_bronze.py             # Ingest scraped data to HDFS Bronze layer
â”‚   â”œâ”€â”€ load_to_dwh.py                  # Load processed data to PostgreSQL
â”‚   â”œâ”€â”€ create_dwh_table.py             # Create One Big Table (OBT) in DWH
â”‚   â”œâ”€â”€ create_datamarts.py             # Create specialized data marts
â”‚   â”œâ”€â”€ populate_datamarts.py           # Populate data marts from OBT
â”‚   â”‚
â”‚   â”œâ”€â”€ data_csv_files/                 # Intermediate CSV files
â”‚   â”œâ”€â”€ __pycache__/                    # Python cache files
â”‚   â””â”€â”€ datalake-hdfs-commands.ipynb    # HDFS setup and management
â”‚
â”œâ”€â”€ postgres_data/                      # Persistent PostgreSQL data volume
â”‚
â”œâ”€â”€ logs/                               # Airflow logs and execution history
â”‚
â”œâ”€â”€ datasets_analysis_reports/          # EDA notebooks and analysis reports
â”‚
â”œâ”€â”€ ML_model/                           # Machine Learning components (Optional)
â”‚   â”œâ”€â”€ knn_recommender.py              # KNN recommendation model
â”‚   â””â”€â”€ train_model.py                  # Model training script
â”‚
â”œâ”€â”€ RAG/                                # AI Assistant components (Optional)
â”‚   â”œâ”€â”€ app.py                          # Flask application for AI Assistant
â”‚   â”œâ”€â”€ ingest_embeddings.py            # Build FAISS index
â”‚   â””â”€â”€ faiss_index.idx                 # FAISS vector index
â”‚
â”œâ”€â”€ docker-compose.yaml                 # Multi-service orchestration
â”œâ”€â”€ airflow.Dockerfile                  # Custom Airflow image
â”œâ”€â”€ jupyter.Dockerfile                  # Custom PySpark/Jupyter image
â”œâ”€â”€ PBI_Analysis_Dashboard.pbix         # Power BI Report file
â””â”€â”€ README.md                           # This file
```

### Pipeline Execution Flow

The Airflow DAG executes tasks in the following order:

1. **Scraping Scripts** â†’ Extract data from 4 real estate platforms
2. **Ingest to Bronze** â†’ Store raw data in HDFS Bronze layer
3. **Cleaning Layer** â†’ Clean and standardize data (Silver layer)
4. **Transformation Layer** â†’ Feature engineering and enrichment
5. **Create DWH Table** â†’ Set up One Big Table schema
6. **Load to DWH** â†’ Load processed data to PostgreSQL
7. **Create & Populate Data Marts** â†’ Build specialized marts for BI and ML

---

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following installed on your system:

- **Docker** (version 20.10+)
- **Docker Compose** (version 1.29+)
- **Git**
- At least **16GB RAM** and **50GB free disk space** for optimal performance

### Installation Steps

#### 1. Clone the Repository

```bash
git clone https://github.com/nhahub/NHA-017.git
cd NHA-017
```

#### 2. Build Custom Docker Images

The project uses custom Docker images for the environment.

**Build Jupyter Image (PySpark + Data Science tools):**

```bash
docker compose build jupyter
```

**Build Airflow and all other services:**

```bash
docker compose build
```

#### 3. Start All Services

Use Docker Compose to spin up the entire stack:

```bash
docker compose up -d
```

This command will start:
- Apache Airflow (Webserver, Scheduler, Workers)
- Apache Spark (Master + Workers)
- Hadoop HDFS (NameNode + DataNode)
- PostgreSQL (Data Warehouse)
- Jupyter Notebook (PySpark environment)

**Wait 2-3 minutes** for all services to initialize.

#### 4. Verify Services

Check that all containers are running:

```bash
docker compose ps
```

You should see all services in the "Up" state.

#### 5. Access Airflow

Navigate to the Airflow UI:

```
http://localhost:8082
```

**Login credentials:**
- Username: `airflow`
- Password: `airflow`

#### 6. Run the Pipeline

Once logged into Airflow:
1. Locate the real_estate_etl pipeline DAG in the DAGs list
2. Enable the DAG by toggling the switch on the left
3. Click the "Play" button to trigger the pipeline manually
4. Monitor execution in the Graph or Grid view

The pipeline will automatically:
- Scrape data from 4 real estate platforms
- Process through Bronze â†’ Silver â†’ Gold layers
- Load the final data into PostgreSQL Data Warehouse

---

## ğŸ“Š Usage Guide

### Running the Data Pipeline

The entire pipeline is automated through Airflow. Simply:

1. **Access Airflow UI:** Navigate to `http://localhost:8082`
2. **Login:** Use username `airflow` and password `airflow`
3. **Enable DAG:** Toggle the switch next to real_estate_etl pipeline DAG
4. **Trigger Pipeline:** Click the "Play" button to start execution
5. **Monitor Progress:** Watch tasks execute in real-time through the Graph or Grid view

The pipeline automatically handles:
- Web scraping from 4 platforms (PropertyFinder, Bayut, Dubizzle, Fazwaz)
- Data ingestion to Bronze layer (HDFS)
- Cleaning and transformation (Silver layer)
- Loading to PostgreSQL Data Warehouse (Gold layer)
- Creating and populating data marts

### Building Dashboards

**Power BI:**
1. Open `PBI_Analysis_Dashboard.pbix`
2. Update the data source connection to your PostgreSQL instance
3. Refresh data to see live insights from your data warehouse

**Apache Superset (if configured):**
1. Connect to PostgreSQL warehouse
2. Use the `analysis_mart` table for optimized BI queries
3. Create custom dashboards and visualizations

---

## ğŸ“ˆ Visualizations & Dashboards

### Power BI Reports

<img width="2129" height="1199" alt="Power BI Dashboard 1" src="https://github.com/user-attachments/assets/bccdb3d3-e558-4f4e-a2a2-bd65dd9ad459" />

<img width="2444" height="1310" alt="Power BI Dashboard 2" src="https://github.com/user-attachments/assets/34493a31-caff-45b4-9b83-19d8a4bb7ceb" />

<img width="2436" height="1314" alt="Power BI Dashboard 3" src="https://github.com/user-attachments/assets/834b0d62-8cf0-4c3e-be1d-a984446a5011" />

### Machine Learning Recommendation System

<img width="975" height="467" alt="ML Recommender 1" src="https://github.com/user-attachments/assets/ee9ef158-b689-4ed0-bd2b-72fc4ed2c9de" />

<img width="975" height="463" alt="ML Recommender 2" src="https://github.com/user-attachments/assets/9816909e-de00-4893-9654-5cce9501be89" />

<img width="975" height="467" alt="ML Recommender 3" src="https://github.com/user-attachments/assets/0f561118-639f-4432-92ca-b7d4403d53c1" />

<img width="975" height="416" alt="ML Recommender 4" src="https://github.com/user-attachments/assets/443550b6-32c2-4cdc-9171-80b655d997da" />

### Superset Visualizations

<img width="975" height="363" alt="Superset Dashboard 1" src="https://github.com/user-attachments/assets/397e7f93-2de6-41f9-bb04-f14b1f0a7a3e" />

<img width="975" height="359" alt="Superset Dashboard 2" src="https://github.com/user-attachments/assets/0366bc28-7e01-4377-8ad1-bd4829d1543e" />

<img width="975" height="324" alt="Superset Dashboard 3" src="https://github.com/user-attachments/assets/c2c5b935-0367-48d0-a9db-a8eebd0fd7ed" />

### AI Assistant Chatbot

<img width="2559" height="429" alt="AI Chatbot 1" src="https://github.com/user-attachments/assets/d657bf34-2cc9-4c4d-92d5-3304ad213b03" />

<img width="2559" height="858" alt="AI Chatbot 2" src="https://github.com/user-attachments/assets/53f69c6a-fa4e-46c5-9f43-ae8d29a6a2c0" />

<img width="2559" height="1461" alt="AI Chatbot 3" src="https://github.com/user-attachments/assets/b74ef525-5fca-42be-823a-27af98bc1578" />

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request
---

<div align="center">

**â­ Star this repo if you find it useful!**

Made with â¤ï¸ for the Egyptian real estate market

</div>
