{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36528dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfa94cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"merging_all_datasets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1e81702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bayut = spark.read.parquet(\"hdfs://namenode:9000/datalake/gold/transformed_bayut\")\n",
    "df_dubbizle =  spark.read.parquet(\"hdfs://namenode:9000/datalake/gold/transformed_dubbizle\")\n",
    "df_fazwaz=  spark.read.parquet(\"hdfs://namenode:9000/datalake/gold/transformed_fazwaz\")\n",
    "df_propertyfinder =  spark.read.parquet(\"hdfs://namenode:9000/datalake/gold/transformed_propertyfinder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4eb3c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- price_per_sqm: double (nullable = true)\n",
      " |-- Jacuzzi: integer (nullable = true)\n",
      " |-- Garden: integer (nullable = true)\n",
      " |-- Balcony: integer (nullable = true)\n",
      " |-- Pool: integer (nullable = true)\n",
      " |-- Parking: integer (nullable = true)\n",
      " |-- Gym: integer (nullable = true)\n",
      " |-- Maids_Quarters: integer (nullable = true)\n",
      " |-- Spa: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- price_per_sqm: double (nullable = true)\n",
      " |-- Jacuzzi: integer (nullable = true)\n",
      " |-- Garden: integer (nullable = true)\n",
      " |-- Balcony: integer (nullable = true)\n",
      " |-- Pool: integer (nullable = true)\n",
      " |-- Parking: integer (nullable = true)\n",
      " |-- Gym: integer (nullable = true)\n",
      " |-- Maids_Quarters: integer (nullable = true)\n",
      " |-- Spa: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- price_per_sqm: double (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- Jacuzzi: integer (nullable = true)\n",
      " |-- Garden: integer (nullable = true)\n",
      " |-- Balcony: integer (nullable = true)\n",
      " |-- Pool: integer (nullable = true)\n",
      " |-- Parking: integer (nullable = true)\n",
      " |-- Gym: integer (nullable = true)\n",
      " |-- Maids_Quarters: integer (nullable = true)\n",
      " |-- Spa: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- Pool: integer (nullable = true)\n",
      " |-- Gym: integer (nullable = true)\n",
      " |-- Garden: integer (nullable = true)\n",
      " |-- Parking: integer (nullable = true)\n",
      " |-- Maids_Quarters: integer (nullable = true)\n",
      " |-- Jacuzzi: integer (nullable = true)\n",
      " |-- Balcony: integer (nullable = true)\n",
      " |-- Spa: integer (nullable = true)\n",
      " |-- price_per_sqm: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bayut.printSchema()\n",
    "df_dubbizle.printSchema()\n",
    "df_fazwaz.printSchema()\n",
    "df_propertyfinder.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a57e5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing Bayut...\n",
      "Standardizing Dubbizle...\n",
      "Standardizing Fazwaz...\n",
      "Standardizing PropertyFinder...\n",
      "Location standardization complete for all sources.\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATED LOGIC: FAZWAZ & PROPERTYFINDER DEEP EXTRACTION ---\n",
    "\n",
    "from pyspark.sql.functions import split, trim, col, when, size, regexp_replace, lit, element_at, slice, array_join, lower\n",
    "\n",
    "# 1. df_bayut processing:\n",
    "print(\"Standardizing Bayut...\")\n",
    "df_bayut = df_bayut.withColumnRenamed(\"region\", \"city\") \\\n",
    "                   .withColumn(\"region\", col(\"location\"))\n",
    "\n",
    "# 2. df_dubbizle processing:\n",
    "def split_dubbizle_location(df):\n",
    "    print(\"Standardizing Dubbizle...\")\n",
    "    df_clean = df.withColumn(\"clean_loc\", regexp_replace(col(\"location\"), \"[.•]+$\", \"\"))\n",
    "    \n",
    "    df_with_city = df_clean.withColumn(\n",
    "        \"city\",\n",
    "        when(col(\"source\") == \"dubbizle_Cairo\", \"Cairo\")\n",
    "        .otherwise(\"Alexandria\")\n",
    "    )\n",
    "    \n",
    "    df_split = df_with_city.withColumn(\"split_arr\", split(col(\"clean_loc\"), \",\"))\n",
    "    last_token = trim(element_at(col(\"split_arr\"), -1))\n",
    "    \n",
    "    df_processed = df_split.withColumn(\n",
    "        \"region\",\n",
    "        when(lower(last_token).isin(\"cairo\", \"alexandria\", \"alex\", \"egypt\"), \n",
    "             trim(element_at(col(\"split_arr\"), -2)))\n",
    "        .otherwise(last_token)\n",
    "    )\n",
    "    return df_processed.drop(\"split_arr\", \"clean_loc\")\n",
    "\n",
    "df_dubbizle = split_dubbizle_location(df_dubbizle)\n",
    "\n",
    "# 3. df_fazwaz (Deep Extraction):\n",
    "def split_fazwaz_location(df):\n",
    "    print(\"Standardizing Fazwaz...\")\n",
    "    df_cleaned = df.withColumn(\"location_cleaned\", regexp_replace(col(\"location\"), \", Egypt$\", \"\"))\n",
    "    df_split = df_cleaned.withColumn(\"split_arr\", split(col(\"location_cleaned\"), \",\"))\n",
    "    df_split = df_split.withColumn(\"split_size\", size(col(\"split_arr\")))\n",
    "    \n",
    "    df_split = df_split.withColumn(\"city\", trim(element_at(col(\"split_arr\"), -1)))\n",
    "    df_split = df_split.withColumn(\"temp_region\", trim(element_at(col(\"split_arr\"), -2)))\n",
    "\n",
    "    df_processed = df_split.withColumn(\n",
    "        \"region\",\n",
    "        when(\n",
    "            (col(\"temp_region\").isin(\"New Cairo\", \"New Cairo City\")) & (col(\"split_size\") >= 3),\n",
    "            trim(element_at(col(\"split_arr\"), -3))\n",
    "        ).otherwise(col(\"temp_region\"))\n",
    "    )\n",
    "    \n",
    "    return df_processed.drop(\"location_cleaned\", \"split_arr\", \"split_size\", \"temp_region\")\n",
    "\n",
    "df_fazwaz = split_fazwaz_location(df_fazwaz)\n",
    "\n",
    "# 4. df_propertyfinder (Deep Extraction):\n",
    "print(\"Standardizing PropertyFinder...\")\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\"location_lower\", lower(col(\"location\")))\n",
    "\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\n",
    "    \"city\",\n",
    "    when(col(\"location_lower\").rlike(\"alex|alexandria|alex governorate|elx\"), \"Alexandria\")\n",
    "    .when(col(\"location_lower\").rlike(\n",
    "        \"cairo|giza|nasr city|heliopolis|maadi|rehab|tagamo3|new cairo|madinaty|badr|shorouk|obour|sheikh zayed|zayed|6th october|6 october|october|hadayek october|october gardens|new giza|palm hills|gardenia|zayed dunes|dahshour|الشيخ زايد|٦ أكتوبر|اكتوبر|حدائق اكتوبر\"\n",
    "    ), \"Cairo\")\n",
    "    .when(col(\"location_lower\").rlike(\"north coast|sidi abdel rahman|al alamein|ras al hekm|dabaa\"), \"North Coast\")\n",
    "    .when(col(\"location_lower\").rlike(\"ain sokhna|sokhna|suez|galala\"), \"Sokhna\")\n",
    "    .when(col(\"location_lower\").rlike(\"hurghada|red sea|soma bay|safaga|gouna\"), \"Red Sea\")\n",
    "    .when(col(\"location_lower\").rlike(\"ras sedr|south sinai|dahab|sharm el sheikh\"), \"South Sinai\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\"split_pf\", split(col(\"location\"), \",\"))\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\"split_size\", size(col(\"split_pf\")))\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\"temp_region\", trim(element_at(col(\"split_pf\"), -2)))\n",
    "\n",
    "df_propertyfinder = df_propertyfinder.withColumn(\n",
    "    \"region\", \n",
    "    when(\n",
    "        (col(\"temp_region\").isin(\"New Cairo City\", \"New Cairo\")) & (col(\"split_size\") >= 3),\n",
    "        trim(element_at(col(\"split_pf\"), -3))\n",
    "    )\n",
    "    .otherwise(col(\"temp_region\"))\n",
    ")\n",
    "\n",
    "df_propertyfinder = df_propertyfinder.drop(\"location_lower\", \"split_pf\", \"split_size\", \"temp_region\")\n",
    "\n",
    "print(\"Location standardization complete for all sources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fdcd005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Merged DataFrame Schema ---\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- price_per_sqm: double (nullable = true)\n",
      " |-- Jacuzzi: integer (nullable = true)\n",
      " |-- Garden: integer (nullable = true)\n",
      " |-- Balcony: integer (nullable = true)\n",
      " |-- Pool: integer (nullable = true)\n",
      " |-- Parking: integer (nullable = true)\n",
      " |-- Gym: integer (nullable = true)\n",
      " |-- Maids_Quarters: integer (nullable = true)\n",
      " |-- Spa: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge all DataFrames\n",
    "final_columns = [\n",
    "    \"title\", \"link\", \"price\", \"location\", \"region\", \"city\", \"area\", \"bedrooms\", \"bathrooms\",\n",
    "    \"latitude\", \"longitude\", \"property_type\", \"source\", \"price_per_sqm\",\n",
    "    \"Jacuzzi\", \"Garden\", \"Balcony\", \"Pool\", \"Parking\", \"Gym\",\n",
    "    \"Maids_Quarters\", \"Spa\", \"description\"\n",
    "]\n",
    "\n",
    "def safe_select(df, columns):\n",
    "    existing_cols = df.columns\n",
    "    select_expr = [\n",
    "        col(c) if c in existing_cols else lit(None).cast(StringType()).alias(c) \n",
    "        for c in columns\n",
    "    ]\n",
    "    return df.select(select_expr)\n",
    "\n",
    "df_bayut_ordered = safe_select(df_bayut, final_columns)\n",
    "df_dubbizle_ordered = safe_select(df_dubbizle, final_columns)\n",
    "df_fazwaz_ordered = safe_select(df_fazwaz, final_columns)\n",
    "df_propertyfinder_ordered = safe_select(df_propertyfinder, final_columns)\n",
    "\n",
    "df_all = (\n",
    "    df_bayut_ordered\n",
    "    .unionByName(df_dubbizle_ordered)\n",
    "    .unionByName(df_fazwaz_ordered)\n",
    "    .unionByName(df_propertyfinder_ordered)\n",
    ")\n",
    "\n",
    "df_all = df_all.dropDuplicates()\n",
    "\n",
    "print(\"--- Merged DataFrame Schema ---\")\n",
    "df_all.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1d0a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 'city' column and standardizing 'region'...\n",
      "Applying constraint: region cannot be the same as city...\n",
      "City and Region columns cleaned. Showing new distinct counts:\n",
      "+-----------+-----+\n",
      "|city       |count|\n",
      "+-----------+-----+\n",
      "|Cairo      |10096|\n",
      "|Alexandria |1823 |\n",
      "|Matruh     |459  |\n",
      "|Giza       |262  |\n",
      "|Red Sea    |255  |\n",
      "|Suez       |181  |\n",
      "|North Coast|106  |\n",
      "|Sokhna     |73   |\n",
      "|South Sinai|3    |\n",
      "|Other      |2    |\n",
      "|Aswan      |1    |\n",
      "|Damietta   |1    |\n",
      "+-----------+-----+\n",
      "\n",
      "--- Region Counts (non-null) ---\n",
      "+----------------------------+-----+\n",
      "|region                      |count|\n",
      "+----------------------------+-----+\n",
      "|5th Settlement              |5068 |\n",
      "|New Cairo                   |686  |\n",
      "|Al Rehab                    |496  |\n",
      "|North Coast                 |459  |\n",
      "|Sheikh Zayed                |399  |\n",
      "|Madinaty                    |367  |\n",
      "|Smoha                       |359  |\n",
      "|New Capital City            |348  |\n",
      "|6 October                   |285  |\n",
      "|Shorouk City                |212  |\n",
      "|Mostakbal City - Future City|202  |\n",
      "|South Investors Area        |195  |\n",
      "|Mostakbal City              |188  |\n",
      "|1st Settlement              |183  |\n",
      "|Ain Sukhna                  |181  |\n",
      "|Nasr City                   |172  |\n",
      "|North Investors Area        |151  |\n",
      "|Mokattam                    |146  |\n",
      "|Laurent                     |142  |\n",
      "|Nakheel                     |134  |\n",
      "+----------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- Sample of all three location columns ---\n",
      "+--------------+--------------+-------+------+\n",
      "|location      |region        |city   |source|\n",
      "+--------------+--------------+-------+------+\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|ain sukhna    |Ain Sukhna    |Suez   |bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|north coast   |North Coast   |Matruh |bayut |\n",
      "|madinaty      |Madinaty      |Cairo  |bayut |\n",
      "|ain sukhna    |Ain Sukhna    |Suez   |bayut |\n",
      "|6th of october|6th Of October|Giza   |bayut |\n",
      "|makadi bay    |Makadi Bay    |Red Sea|bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|north coast   |North Coast   |Matruh |bayut |\n",
      "|mokattam      |Mokattam      |Cairo  |bayut |\n",
      "|north coast   |North Coast   |Matruh |bayut |\n",
      "|ain sukhna    |Ain Sukhna    |Suez   |bayut |\n",
      "|sahl hasheesh |Sahl Hasheesh |Red Sea|bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|madinaty      |Madinaty      |Cairo  |bayut |\n",
      "|north coast   |North Coast   |Matruh |bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "|new cairo     |New Cairo     |Cairo  |bayut |\n",
      "+--------------+--------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATED CLEANING & NAME STANDARDIZATION ---\n",
    "\n",
    "from pyspark.sql.functions import initcap, trim, regexp_replace\n",
    "\n",
    "print(\"Cleaning 'city' column and standardizing 'region'...\")\n",
    "\n",
    "# 1. Basic Trim & Formatting\n",
    "df_all = df_all.withColumn(\"city\", initcap(regexp_replace(trim(col(\"city\")), \"[.•]$\", \"\")))\n",
    "df_all = df_all.withColumn(\"region\", initcap(regexp_replace(trim(col(\"region\")), \"[.•]$\", \"\")))\n",
    "\n",
    "# 2. Fix City Names\n",
    "df_all = df_all.withColumn(\"city\", \n",
    "    when(col(\"city\") == \"Alex\", \"Alexandria\")\n",
    "    .otherwise(col(\"city\"))\n",
    ")\n",
    "df_all = df_all.withColumn(\"region\", \n",
    "    when(col(\"region\") == \"Alex\", \"Alexandria\")\n",
    "    .otherwise(col(\"region\"))\n",
    ")\n",
    "\n",
    "# 3. --- NAME STANDARDIZATION STEP ---\n",
    "# This fixes 'The 5th Settlement' and merges 'New Cairo City' -> 'New Cairo'\n",
    "df_all = df_all.withColumn(\"region\",\n",
    "    regexp_replace(col(\"region\"), \"The 5th Settlement\", \"5th Settlement\")\n",
    ")\n",
    "df_all = df_all.withColumn(\"region\",\n",
    "    regexp_replace(col(\"region\"), \"New Cairo City\", \"New Cairo\")\n",
    ")\n",
    "df_all = df_all.withColumn(\"region\",\n",
    "    regexp_replace(col(\"region\"), \"6 October City\", \"6 October\")\n",
    ")\n",
    "df_all = df_all.withColumn(\"region\",\n",
    "    regexp_replace(col(\"region\"), \"Sheikh Zayed City\", \"Sheikh Zayed\")\n",
    ")\n",
    "\n",
    "# 4. Apply Constraint: region != city\n",
    "print(\"Applying constraint: region cannot be the same as city...\")\n",
    "df_all = df_all.withColumn(\"region\",\n",
    "    when(col(\"region\") == col(\"city\"), None) \n",
    "    .otherwise(col(\"region\"))\n",
    ")\n",
    "\n",
    "print(\"City and Region columns cleaned. Showing new distinct counts:\")\n",
    "city_counts_cleaned = df_all.groupBy(\"city\").count().orderBy(\"count\", ascending=False)\n",
    "city_counts_cleaned.show(truncate=False)\n",
    "\n",
    "print(\"--- Region Counts (non-null) ---\")\n",
    "region_counts_cleaned = df_all.filter(col(\"region\").isNotNull()).groupBy(\"region\").count().orderBy(\"count\", ascending=False)\n",
    "region_counts_cleaned.show(truncate=False)\n",
    "\n",
    "print(\"--- Sample of all three location columns ---\")\n",
    "df_all.select(\"location\", \"region\", \"city\", \"source\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a322f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.withColumn(\"property_type\", lower(col(\"property_type\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48741c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e4fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Spark DataFrame to Pandas DataFrame...\n",
      "Conversion complete. 13262 rows to insert.\n",
      "Connecting to PostgreSQL...\n",
      "Truncating destination table for a fresh load...\n",
      "Preparing data tuples for insertion...\n",
      "Executing batch insert of 13262 rows...\n",
      "Data loaded successfully!\n",
      "Closing connection.\n"
     ]
    }
   ],
   "source": [
    "# Load the final merged DataFrame into PostgreSQL\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "print(\"Converting Spark DataFrame to Pandas DataFrame...\")\n",
    "pdf = df_all.toPandas()\n",
    "print(f\"Conversion complete. {len(pdf)} rows to insert.\")\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": \"real_state_dwh\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"host\": \"postgres_general\", \n",
    "    \"port\": 5432\n",
    "}\n",
    "\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO real_estate_one_big_table (\n",
    "    title, link, price, location, region, city, area, bedrooms, bathrooms, latitude, longitude,\n",
    "    property_type, source, price_per_sqm, Jacuzzi, Garden, Balcony, Pool,\n",
    "    Parking, Gym, Maids_Quarters, Spa, description\n",
    ") VALUES %s\n",
    "ON CONFLICT (link) DO NOTHING; \n",
    "\"\"\"\n",
    "\n",
    "conn = None\n",
    "cur = None\n",
    "try:\n",
    "    print(\"Connecting to PostgreSQL...\")\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    print(\"Truncating destination table for a fresh load...\")\n",
    "    cur.execute(\"TRUNCATE TABLE real_estate_one_big_table;\")\n",
    "\n",
    "    print(\"Preparing data tuples for insertion...\")\n",
    "    data_tuples = [tuple(x) for x in pdf.to_numpy()]\n",
    "\n",
    "    print(f\"Executing batch insert of {len(data_tuples)} rows...\")\n",
    "    execute_values(cur, insert_sql, data_tuples)\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"Data loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    if conn:\n",
    "        conn.rollback()\n",
    "\n",
    "finally:\n",
    "    print(\"Closing connection.\")\n",
    "    if cur:\n",
    "        cur.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
